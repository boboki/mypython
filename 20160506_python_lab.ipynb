{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那酸\n",
      "酸民\n",
      "11\n",
      "那酸\n",
      "酸民\n",
      "民婉\n",
      "婉君\n",
      "君也\n",
      "也可\n",
      "可以\n",
      "以報\n",
      "報名\n",
      "名嗎\n"
     ]
    }
   ],
   "source": [
    "input_sentence = '那酸民婉君也可以報名嗎'\n",
    "print(input_sentence[0:2])\n",
    "print(input_sentence[1:3])\n",
    "print(len(input_sentence))\n",
    "\n",
    "#for \n",
    "sentence = input_sentence\n",
    "for i in range(0, len(sentence) -1 ):\n",
    "    print(sentence[i:i+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那酸民\n",
      "酸民婉\n",
      "11\n",
      "那酸民\n",
      "酸民婉\n",
      "民婉君\n",
      "婉君也\n",
      "君也可\n",
      "也可以\n",
      "可以報\n",
      "以報名\n",
      "報名嗎\n"
     ]
    }
   ],
   "source": [
    "input_sentence = '那酸民婉君也可以報名嗎'\n",
    "print(input_sentence[0:3])\n",
    "print(input_sentence[1:4])\n",
    "print(len(input_sentence))\n",
    "\n",
    "#for \n",
    "sentence = input_sentence\n",
    "for i in range(0, len(sentence) -2 ):\n",
    "    print(sentence[i:i+3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那酸\n",
      "酸民\n",
      "民婉\n",
      "婉君\n",
      "君也\n",
      "也可\n",
      "可以\n",
      "以報\n",
      "報名\n",
      "名嗎\n",
      "那酸民\n",
      "酸民婉\n",
      "民婉君\n",
      "婉君也\n",
      "君也可\n",
      "也可以\n",
      "可以報\n",
      "以報名\n",
      "報名嗎\n"
     ]
    }
   ],
   "source": [
    "input_sentence = '那酸民婉君也可以報名嗎'\n",
    "\n",
    "def ngram(input_sentence, n = 2):\n",
    "    sentence = input_sentence\n",
    "    for i in range(0, len(sentence) - n + 1 ):\n",
    "        print(sentence[i:i+n])\n",
    "        \n",
    "ngram(input_sentence)\n",
    "ngram(input_sentence, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'c': 1, 'b': 2}\n",
      "dict_items([('a', 2), ('c', 1), ('b', 2)])\n"
     ]
    }
   ],
   "source": [
    "ary = ['a' , 'b', 'a', 'c', 'b']\n",
    "dic = {}\n",
    "for ele in ary:\n",
    "    if ele not in dic:\n",
    "        dic[ele] = 1\n",
    "    else:\n",
    "        dic[ele] = dic[ele] + 1\n",
    "print(dic)\n",
    "print(dic.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', 1), ('a', 2), ('b', 2)]\n",
      "[('a', 2), ('b', 2), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "swd = sorted(dic.items(), key = operator.itemgetter(1))\n",
    "print(swd)\n",
    "\n",
    "swd = sorted(dic.items(), key = operator.itemgetter(1), reverse=True)\n",
    "print(swd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter\n",
    "https://docs.python.org/2/library/collections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 2, 'b': 2, 'c': 1})\n",
      "[('a', 2), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter()\n",
    "for ele in ary:\n",
    "    c[ele] += 1\n",
    "print(c)\n",
    "\n",
    "print(c.most_common(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(input_sentence, n = 2):\n",
    "    dic = {}\n",
    "    sentence = input_sentence\n",
    "    for i in range(0, len(sentence) - n + 1 ):\n",
    "        segment = sentence[i:i+n]\n",
    "        if segment not in dic:\n",
    "            dic[segment] = 1\n",
    "        else:\n",
    "            dic[segment] = dic[segment] + 1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'可以': 1, '那酸': 1, '名嗎': 1, '報名': 1, '婉君': 1, '君也': 1, '酸民': 1, '民婉': 1, '也可': 1, '以報': 1}\n",
      "否跟 1\n",
      "昨天 1\n",
      "氣一 1\n",
      "天天 2\n",
      "跟昨 1\n",
      "天氣 2\n",
      "一樣 1\n",
      "今天 1\n",
      "氣是 1\n",
      "是否 1\n"
     ]
    }
   ],
   "source": [
    "print(ngram(input_sentence, 2))\n",
    "dic = ngram(\"今天天氣是否跟昨天天氣一樣\", 2)\n",
    "for ele in dic:\n",
    "    print(ele, dic[ele])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(input_sentence, n = 2):\n",
    "    c = Counter()\n",
    "    sentence = input_sentence\n",
    "    for i in range(0, len(sentence) - n + 1 ):\n",
    "        segment = sentence[i:i+n]\n",
    "        c[segment] += 1        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'一樣': 1,\n",
       " '今天': 1,\n",
       " '否跟': 1,\n",
       " '天天': 2,\n",
       " '天氣': 2,\n",
       " '昨天': 1,\n",
       " '是否': 1,\n",
       " '氣一': 1,\n",
       " '氣是': 1,\n",
       " '跟昨': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram(\"今天天氣是否跟昨天天氣一樣\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    697\n",
       "link        697\n",
       "summary     697\n",
       "time        697\n",
       "title       697\n",
       "view_cnt    697\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3 as lite\n",
    "import pandas as pd\n",
    "with lite.connect('news.sqlite') as db:\n",
    "    df = pd.read_sql_query('SELECT * FROM news_entry;', db)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【不只鄭捷】羅螢雪今晚《紅心Ａ》大爆料\n",
      "】\n",
      "【\n",
      "？\n",
      ".\n",
      "、\n",
      "《\n",
      "》\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(df.ix[1]['title'])\n",
    "\n",
    "skip_word = '】【？.、《》'\n",
    "for w in skip_word:\n",
    "    print(w)\n",
    "    \n",
    "a = '你好嗎？'\n",
    "print('？' in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大爆\n",
      "羅螢\n",
      "紅心\n",
      "不只\n",
      "心Ａ\n",
      "只鄭\n",
      "今晚\n",
      "螢雪\n",
      "鄭捷\n",
      "雪今\n",
      "爆料\n"
     ]
    }
   ],
   "source": [
    "# 如 ngram的詞中有出現skip_word，就捨棄該詞\n",
    "c = ngram(df.ix[1]['title'], 2)\n",
    "for ele in c:\n",
    "    ary = []\n",
    "    for w in skip_word:\n",
    "        if w in ele: \n",
    "            ary.append(w)\n",
    "    if len(ary) == 0:\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大爆\n",
      "羅螢\n",
      "紅心\n",
      "不只\n",
      "心Ａ\n",
      "只鄭\n",
      "今晚\n",
      "螢雪\n",
      "鄭捷\n",
      "雪今\n",
      "爆料\n"
     ]
    }
   ],
   "source": [
    "# for包含式\n",
    "c = ngram(df.ix[1]['title'], 2)\n",
    "for ele in c:\n",
    "    invalid = len([w for w in skip_word if w in ele])\n",
    "    if invalid == 0:\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_word = '】【？.、「 」　'\n",
    "def ngram2(input_sentence, n = 2):\n",
    "    c = Counter()\n",
    "    sentence = input_sentence\n",
    "    for i in range(0, len(sentence) - n + 1 ):\n",
    "        segment = sentence[i:i+n]\n",
    "        invalid = len([w for w in skip_word if w in segment]) \n",
    "        if invalid ==0:\n",
    "            c[segment] += 1        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "巡大 1\n",
      "大隊 1\n",
      "隊長 1\n",
      "海巡 1\n",
      "領伙 1\n",
      "詐領 1\n",
      "前海 1\n",
      "伙食 1\n",
      "食費 1\n",
      "秘書 1\n"
     ]
    }
   ],
   "source": [
    "title =  df.ix[0]['title']\n",
    "c = ngram2(title, 2)\n",
    "for ele in c:\n",
    "    print(ele, c[ele])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = df['title']\n",
    "c2 = Counter()\n",
    "for title in titles:\n",
    "    c = ngram2(title, 2)\n",
    "    for ele in c:\n",
    "        c2[ele] += c[ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有片 92\n",
      "鄭捷 75\n",
      "壹週 58\n",
      "週刊 58\n",
      "槍決 28\n",
      "TI 25\n",
      "廣R 24\n",
      "央廣 24\n",
      "RT 24\n",
      "20 21\n",
      "死刑 18\n",
      "00 15\n",
      "台灣 14\n",
      "Pa 12\n",
      "瑩雪 12\n",
      "羅瑩 12\n",
      "伏法 12\n",
      "這樣 11\n",
      "更新 11\n",
      "學生 11\n",
      "公庫 11\n",
      "le 11\n",
      "影片 11\n",
      "52 11\n",
      "ay 10\n",
      "pp 10\n",
      "Ap 10\n",
      "新聞 9\n",
      "中國 9\n",
      "國台 9\n"
     ]
    }
   ],
   "source": [
    "dic = c2.most_common(30)\n",
    "for ele in dic:\n",
    "    print(ele[0], ele[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = df['title']\n",
    "c2 = Counter()\n",
    "for title in titles:\n",
    "    c = ngram2(title, 3)\n",
    "    for ele in c:\n",
    "        c2[ele] += c[ele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "壹週刊 58\n",
      "RTI 24\n",
      "央廣R 24\n",
      "廣RT 24\n",
      "羅瑩雪 12\n",
      "App 10\n",
      "ppl 9\n",
      "鄭捷伏 9\n",
      "520 9\n",
      "ple 9\n",
      "捷伏法 9\n",
      "Pay 8\n",
      "速槍決 8\n",
      "火速槍 8\n",
      "國台辦 8\n",
      "捷遺體 7\n",
      "鄭捷遺 7\n",
      "應曉薇 6\n",
      "決鄭捷 6\n",
      "Pro 5\n",
      "瑩雪： 5\n",
      "MVP 5\n",
      "女主播 5\n",
      "民進黨 5\n",
      "鄭捷槍 5\n",
      "大雷雨 5\n",
      "槍決鄭 5\n",
      "洪文棟 5\n",
      "台辦： 5\n",
      "捷槍決 5\n"
     ]
    }
   ],
   "source": [
    "dic = c2.most_common(30)\n",
    "for ele in dic:\n",
    "    print(ele[0], ele[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python one line code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLE\n",
      "['APPLE', 'BANANA', 'COKE', 'DRINK']\n",
      "['APPLE', 'BANANA', 'COKE', 'DRINK']\n"
     ]
    }
   ],
   "source": [
    "a = ['apple', 'banana', 'coke', 'Drink']\n",
    "print('apple'.upper())\n",
    "\n",
    "ary = []\n",
    "for w in a:\n",
    "    ary.append(w.upper())\n",
    "print(ary)\n",
    "\n",
    "print([w.upper() for w in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COKE']\n",
      "['COKE']\n"
     ]
    }
   ],
   "source": [
    "a = ['apple', 'banana', 'coke', 'Drink']\n",
    "\n",
    "ary = []\n",
    "for w in a:\n",
    "    if 'c' in w:\n",
    "        ary.append(w.upper())\n",
    "print(ary)\n",
    "\n",
    "print([w.upper() for w in a if 'c' in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天下午4時許\n",
      "台北市民生東路三段一處辦公大樓\n",
      "2名男子墜樓身亡\n",
      "2人墜落人行道上\n",
      "頭骨破裂腦漿四溢\n",
      "當時有一對老夫婦剛好路過嚇了一跳\n",
      "被墜落2人波及\n",
      "老翁夫婦腿部沾染到散落的遺體\n",
      "2人驚嚇過度推擠受傷送醫治療\n",
      "警方目前已封鎖現場\n",
      "清查兩名死者身分\n",
      "並調閱大樓監視器釐清墜樓原因\n",
      "突發中心林金聖\n",
      "黃彥傑／台北報導\n",
      "【想知道更多\n",
      "一定要看……】【有片】北市2人一起墜樓　皆傷重死亡2男同時墜樓身亡\n",
      "警方封鎖現場\n",
      "黃彥傑攝遭波及的老夫婦受到驚嚇送醫\n",
      "黃彥傑攝\n"
     ]
    }
   ],
   "source": [
    "summary = df.ix[3]['summary']\n",
    "# print(summary)\n",
    "import re\n",
    "for sentence in re.split('，|：|。|、|/|；|（|）', summary):\n",
    "    if sentence.strip() != '':\n",
    "        print(sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = df.ix[0]['summary']\n",
    "c2 = Counter()\n",
    "import re\n",
    "for sentence in re.split('，|：|。|、|/|；|（|）', summary):\n",
    "    if sentence.strip() != '':\n",
    "        c = ngram2(sentence.strip())\n",
    "        c2 = c2 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "貨行 4\n",
      "王倫 4\n",
      "雜貨 4\n",
      "大隊 4\n",
      "萬元 4\n",
      "倫俊 4\n",
      "01 3\n",
      "20 3\n",
      "六二 3\n",
      "伙食 3\n",
      "被告 3\n"
     ]
    }
   ],
   "source": [
    "for ele in  c2.most_common(30):\n",
    "    if ele[1] >= 3:\n",
    "        print(ele[0], ele[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黑白系統則可捕捉細節\n",
      "黑白則可捕捉細節\n",
      "黑白\n",
      "則可捕捉細節\n",
      "黑白則可捕捉細節\n",
      "黑白則可細節\n",
      "黑白則可細節\n"
     ]
    }
   ],
   "source": [
    "# 字串replace by ''\n",
    "a = '黑白系統則可捕捉細節'\n",
    "print(a)\n",
    "\n",
    "print(a.replace('系統',''))\n",
    "\n",
    "for ele in a.split('系統'):\n",
    "    print(ele)\n",
    "\n",
    "print(''.join(a.split('系統')))\n",
    "\n",
    "keywords = ['系統', '捕捉']\n",
    "for keyword in keywords:\n",
    "    a = ''.join(a.split(keyword))\n",
    "print(a)\n",
    "\n",
    "b = '黑白系統則可捕捉細節'\n",
    "for keyword in keywords:\n",
    "    b = b.replace(keyword,'')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "則可細節\n"
     ]
    }
   ],
   "source": [
    "def removeKey(sentence, keywords):\n",
    "    for keyword in keywords:\n",
    "        sentence = ''.join(sentence.split(keyword))\n",
    "    return sentence    \n",
    "print(removeKey('黑白系統則可捕捉細節',  ['系統', '捕捉', '黑白']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "到底要不要\n"
     ]
    }
   ],
   "source": [
    "print(removeKey('到底要不要大巨蛋',  ['大巨蛋']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a@b@c@d\n"
     ]
    }
   ],
   "source": [
    "a = [\"a\", \"b\", \"c\", \"d\"]\n",
    "print('@'.join(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實作長詞優先演算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "summaries = df['summary']\n",
    "delimiter = '，|：|。|、|/|；|（|）|(|)|「|」|」|《|》|／|【|……'\n",
    "keywords = []\n",
    "\n",
    "for i in range(4, 1, -1): # i= 4, 3, 2\n",
    "    csummary = Counter()\n",
    "    for summary in summaries[0:20]:        \n",
    "        for sentence in re.split(delimiter, summary): # 斷句\n",
    "            if sentence is not None and sentence.strip() != '':\n",
    "                c = ngram2(removeKey(sentence.strip(), keywords), i) #取ngram\n",
    "                csummary = csummary + c\n",
    "    print(i)\n",
    "    for ele in csummary:\n",
    "        if csummary[ele] >=10:\n",
    "            if re.match(u\"[\\u4e00-\\u9fa5]\", ele): # 取中文\n",
    "                keywords.append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北報導\n",
      "學雜費\n",
      "羅瑩雪\n",
      "會議\n",
      "死刑\n",
      "萬元\n",
      "人員\n",
      "更新\n",
      "表示\n",
      "報導\n",
      "中國\n",
      "學生\n",
      "時間\n",
      "蘋果\n",
      "執行\n",
      "進行\n",
      "昨晚\n",
      "銀行\n",
      "鄭捷\n",
      "翻攝\n",
      "老公\n"
     ]
    }
   ],
   "source": [
    "for ele in keywords:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "summaries = df['summary']\n",
    "delimiter = '，|：|。|、|/|；|（|）|(|)|「|」|」|《|》|／|【|……'\n",
    "keywords = []\n",
    "\n",
    "def getSentenceNgram(summary):\n",
    "    csummary = Counter()\n",
    "    for sentence in re.split(delimiter, summary): # 斷句\n",
    "        if sentence is not None and sentence.strip() != '':\n",
    "                c = ngram2(removeKey(sentence.strip(), keywords), i) #取ngram\n",
    "                csummary = csummary + c\n",
    "    return csummary\n",
    "\n",
    "for i in range(4, 1, -1):\n",
    "    coverall = Counter()\n",
    "    for summary in summaries[0:20]:        \n",
    "        c = getSentenceNgram(summary) \n",
    "        coverall = coverall  + c\n",
    "    for ele in coverall:\n",
    "        if coverall[ele] >=10:\n",
    "            if re.match(u\"[\\u4e00-\\u9fa5]\", ele):\n",
    "                keywords.append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北報導\n",
      "學雜費\n",
      "羅瑩雪\n",
      "會議\n",
      "死刑\n",
      "人員\n",
      "更新\n",
      "鄭捷\n",
      "表示\n",
      "報導\n",
      "中國\n",
      "學生\n",
      "時間\n",
      "蘋果\n",
      "執行\n",
      "進行\n",
      "昨晚\n",
      "銀行\n",
      "翻攝\n",
      "萬元\n",
      "老公\n"
     ]
    }
   ],
   "source": [
    "for ele in keywords:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'b': 100, 'a': 1})\n",
      "a\n",
      "b\n",
      "Counter({'a': 2, 'b': 2})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'a': 3, 'b': 102})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter()\n",
    "c['a'] += 1\n",
    "c['b'] += 100\n",
    "print(c)\n",
    "for ele in c:\n",
    "    print(ele)\n",
    "    \n",
    "c2 = Counter()\n",
    "c2['a'] += 2\n",
    "c2['b'] += 2\n",
    "\n",
    "print(c2)\n",
    "c+ c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jieba\n",
    "https://pypi.python.org/packages/f6/86/9e721cc52075a07b7d07eb12bcb5dde771d35332a3dae1e14ae4290a197a/jieba-0.38.zip\n",
    "* pip install jieba-0.38.zip\n",
    "* or pip install jieba\n",
    "* C:\\Anaconda2\\Lib\\site-packages\\jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "海巡南區巡防局疑因所轄六二海巡隊前大隊長王倫俊涉詐領同仁伙食費，經報請偵辦，屏東地檢署昨天指揮調查、廉政人員兵分五路，搜索王倫俊住處、六二大隊部、廠商的食品雜貨行等，查扣相關帳冊、收據等，發現王等疑每月挪用伙食費2萬元作為虛列費用。因涉案人犯後態度多坦承，檢方裁定王倫俊30萬元，同隊分隊長吳哲遠5萬元交保；另涉案的雜貨行陳姓負責人等7人均訊後請回。 屏檢指出，王倫俊（現任南巡局秘書）於2014年5月至2015年8月任職六二大隊期間，就部隊勤膳食採購補給業務，於2015年元月起至8月間，指示該大隊勤務中隊的吳哲遠每月自伙食團費中挪用2萬元，但要求採買人員向雜貨行要求開立不實的申購虛列帳目，由雜貨行陳姓婦人開出單據給吳辦理單位核銷，待陳婦取得款項後，再由陳婦退款給吳。 檢方指出，昨天搜索5處處所，並傳喚被告、證人等共10人，認定王、吳均涉詐欺、偽造文書、違反商業會計法等罪嫌。因被告偵訊過程見到提示的帳冊等相關證據後，多坦承配合，認定無羈押必要，將被告裁定交保。（陳宏銘／屏東報導）\n",
      "海巡 南區 巡防 局疑 因所轄 六二 海巡 隊前 大隊 長 王倫俊涉 詐領 同仁 伙食費 ， 經報 請 偵辦 ， 屏東 地檢署 昨天 指揮 調查 、 廉政 人員兵 分 五路 ， 搜索 王倫俊住 處 、 六二 大隊部 、 廠商 的 食品 雜貨行 等 ， 查扣 相關 帳冊 、 收據 等 ， 發現 王等疑 每月 挪用 伙食費 2 萬元作 為 虛列費用 。 因 涉案人 犯後態度 多 坦承 ， 檢方 裁定 王倫俊 30 萬元 ， 同隊 分隊 長 吳哲遠 5 萬元 交保 ； 另 涉案 的 雜貨行 陳 姓 負責人 等 7 人均 訊後請 回 。   屏檢 指出 ， 王倫俊 （ 現任 南巡 局秘書 ） 於 2014 年 5 月 至 2015 年 8 月 任職 六二 大隊 期間 ， 就部 隊勤 膳食 採購 補給 業務 ， 於 2015 年 元月 起至 8 月間 ， 指示 該大隊 勤務 中隊 的 吳哲遠 每月 自 伙食 團費 中 挪用 2 萬元 ， 但 要求 採買 人員向 雜貨行 要求 開立 不實 的 申購 虛列 帳目 ， 由雜 貨行 陳 姓 婦人 開出 單據給 吳辦理 單位 核銷 ， 待陳婦 取得 款項 後 ， 再 由陳婦 退款 給吳 。   檢方 指出 ， 昨天 搜索 5 處處 所 ， 並傳喚 被告 、 證人 等 共 10 人 ， 認定 王 、 吳均 涉詐 欺 、 偽造 文書 、 違反 商業會 計法 等 罪嫌 。 因 被告 偵訊 過程 見 到 提示 的 帳冊 等 相關 證據 後 ， 多 坦承 配合 ， 認定 無 羈押 必要 ， 將 被告 裁定 交保 。 （ 陳 宏銘 ／ 屏東 報導 ） "
     ]
    }
   ],
   "source": [
    "summary = df.ix[0]['summary']\n",
    "print(summary)\n",
    "\n",
    "jieba.add_word('伙食費')\n",
    "\n",
    "for w in jieba.cut(summary):\n",
    "    print(w, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 大/ 巨蛋/ 案/ 對/ 市府/ 同仁/ 下/ 封口/ 封口令/ 口令/ / / 柯/ P/ 否/ 認\n",
      "Precise Mode: 大/ 巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 柯/ P/ 否認\n",
      "Precise Mode: 大/ 巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 柯/ P/ 否認\n"
     ]
    }
   ],
   "source": [
    "seg_list=jieba.cut(\"大巨蛋案對市府同仁下封口令？柯P否認\",cut_all=True)\n",
    "print(\"Full Mode:\",\"/ \".join(seg_list))\n",
    "\n",
    "\n",
    "seg_list=jieba.cut(\"大巨蛋案對市府同仁下封口令？柯P否認\",cut_all=False)\n",
    "print(\"Precise Mode:\",\"/ \".join(seg_list))\n",
    "\n",
    "seg_list=jieba.cut(\"大巨蛋案對市府同仁下封口令？柯P否認\")\n",
    "print(\"Precise Mode:\",\"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precise Mode: 大/ 巨蛋/ 案對/ 市府/ 同仁/ 下/ 封口令/ ？/ 柯P/ 否認\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('柯P')\n",
    "seg_list=jieba.cut(\"大巨蛋案對市府同仁下封口令？柯P否認\",cut_all=False)\n",
    "print(\"Precise Mode:\",\"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大 a\n",
      "巨蛋 n\n",
      "案 ng\n",
      "對 p\n",
      "市府 n\n",
      "同仁 nr\n",
      "下 f\n",
      "封口令 n\n",
      "？ x\n",
      "柯P n\n",
      "否認 v\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('柯P', 100, 'n')\n",
    "import jieba.posseg as pseg\n",
    "words=pseg.cut(\"大巨蛋案對市府同仁下封口令？柯P否認\")\n",
    "for w in words:\n",
    "    print(w.word,w.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大 0 1\n",
      "巨蛋 1 3\n",
      "案對 3 5\n",
      "市府 5 7\n",
      "同仁 7 9\n",
      "下 9 10\n",
      "封口令 10 13\n",
      "？ 13 14\n",
      "柯P 14 16\n",
      "否認 16 18\n"
     ]
    }
   ],
   "source": [
    "# 字詞、起始位置、結束位置\n",
    "sentence = '大巨蛋案對市府同仁下封口令？柯P否認'\n",
    "words=jieba.tokenize(sentence)\n",
    "for tw in words:\n",
    "    print(tw[0],tw[1],tw[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "巨蛋,封口令\n",
      "巨蛋,封口令,柯P,市府\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse \n",
    "sentence = '大巨蛋案對市府同仁下封口令？柯P否認'\n",
    "tags=jieba.analyse.extract_tags(sentence,2)\n",
    "print(\",\".join(tags))\n",
    "\n",
    "tags=jieba.analyse.extract_tags(sentence,5,allowPOS=['n'])\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "1 3\n",
      "2 4\n",
      "3 5\n",
      "4 6\n"
     ]
    }
   ],
   "source": [
    "a = [2,3,4,5,6]\n",
    "cnt = 0 \n",
    "for ele in a:\n",
    "    print(cnt, ele)\n",
    "    cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "1 3\n",
      "2 4\n",
      "3 5\n",
      "4 6\n"
     ]
    }
   ],
   "source": [
    "for idx, ele in enumerate(a):\n",
    "    print(idx, ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "[('，', 134), ('的', 50), ('\\xa0', 49), ('。', 43), ('「', 27), ('」', 27), ('、', 26), (' ', 24), ('：', 15), ('（', 14), ('）', 14), ('鄭捷', 13), ('2', 13), ('報導', 11), ('雲', 9), ('！', 9), ('等', 9), ('母親', 9), ('黃', 9), ('月', 8)]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "c = Counter()\n",
    "summaries = df['summary'][0:10]\n",
    "for idx, summary in enumerate(summaries):\n",
    "    for ele in jieba.cut(summary.encode('utf-8')):\n",
    "        c[ele] +=1\n",
    "    print(idx)\n",
    "print(c.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "for ele in c.most_common(300):\n",
    "    if len(ele[0]) >= 2 and re.match(u\"[\\u4e00-\\u9fa5]\", ele[0]):\n",
    "        #print(ele[0], ele[1])\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a, b = 2,3\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 0.0, 0.0)\n",
      "(0.3333333333333333, 0.0, 0.0)\n",
      "(0.3333333333333333, 0.0, 0.0)\n",
      "(0.6666666666666666, 0.40546510810816438, 0.27031007207210955)\n",
      "(0.3333333333333333, 0.40546510810816438, 0.13515503603605478)\n",
      "(0.3333333333333333, 1.0986122886681098, 0.36620409622270322)\n"
     ]
    }
   ],
   "source": [
    "# tfidf，關鍵字重要性\n",
    "import scipy as sp\n",
    "def tfidf(t,d,D):\n",
    "    tf=float(d.count(t))/sum(d.count(w) for w in set(d))\n",
    "    idf=sp.log(float(len(D))/(len([doc for doc in D if t in doc])))\n",
    "    return tf, idf, tf*idf\n",
    "\n",
    "a,abb,abc=[\"a\"],[\"a\",\"b\",\"b\"],[\"a\",\"b\",\"c\"]\n",
    "\n",
    "D=[a,abb,abc]\n",
    "print(tfidf('a', a, D))\n",
    "print(tfidf('a', abb, D))\n",
    "print(tfidf('a', abc, D))\n",
    "\n",
    "print(tfidf('b', abb, D))\n",
    "print(tfidf('b', abc, D))\n",
    "\n",
    "print(tfidf('c', abc, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天 天氣 真好 今天 晚上 會 下雨 "
     ]
    }
   ],
   "source": [
    "a =  '今天天氣真好'\n",
    "b =  '今天晚上會下雨'\n",
    "for ele in jieba.cut(a):\n",
    "    print(ele,end=' ') \n",
    "    \n",
    "for ele in jieba.cut(b):\n",
    "    print(ele,end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天 天氣 真好', '今天 晚上 會 下雨']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "corpus.append(' '.join(jieba.cut(a)))\n",
    "corpus.append(' '.join(jieba.cut(b)))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下雨 今天 天氣 晚上 真好 \n",
      "[[0 1 1 0 1]\n",
      " [1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "\n",
    "word=vectorizer.get_feature_names()\n",
    "for w in word:\n",
    "    print(w, end=' ')\n",
    "print()\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['【 更新 】 柯P ： 洪智坤 洩漏 公文 案還 沒 看到 公文 今處理', '留洪智坤柯 ： 殘障 求職 不易', '人事 處議 處 洪智坤 柯P ： 不 清楚 議處 結果']\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "ary=['【更新】柯P：洪智坤洩漏公文案還沒看到公文今處理',\n",
    "     '留洪智坤柯：殘障求職不易',\n",
    "     '人事處議處洪智坤柯P：不清楚議處結果']\n",
    "corpus=[]\n",
    "for title in ary:\n",
    "    corpus.append(' '.join(jieba.cut(title)))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不易 人事 今處理 公文 更新 柯p 案還 殘障 求職 洩漏 洪智坤 清楚 留洪智坤柯 看到 結果 處議 議處 \n",
      "[[0 0 1 2 1 1 1 0 0 1 1 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "\n",
    "word=vectorizer.get_feature_names()\n",
    "for w in word:\n",
    "    print(w,end=' ') \n",
    "print()\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.add_word('洪智坤')\n",
    "import jieba \n",
    "ary=['【更新】柯P：洪智坤洩漏公文案還沒看到公文今處理',\n",
    "     '留洪智坤柯：殘障求職不易',\n",
    "     '人事處議處洪智坤柯P：不清楚議處結果']\n",
    "corpus=[]\n",
    "for title in ary:\n",
    "    corpus.append(' '.join(jieba.cut(title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不易 人事 今處理 公文 更新 柯p 案還 殘障 求職 洩漏 洪智坤 清楚 看到 結果 處議 議處 \n",
      "[[0 0 1 2 1 1 1 0 0 1 1 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "\n",
    "word=vectorizer.get_feature_names()\n",
    "for w in word:\n",
    "    print(w,end=' ') \n",
    "print()\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不易 人事 今處理 公文 更新 柯p 案還 殘障 求職 洩漏 洪智坤 清楚 看到 結果 處議 議處 \n",
      "[[ 0.          0.          0.31738473  0.63476946  0.31738473  0.24137927\n",
      "   0.31738473  0.          0.          0.31738473  0.18745253  0.\n",
      "   0.31738473  0.          0.          0.        ]\n",
      " [ 0.54645401  0.          0.          0.          0.          0.          0.\n",
      "   0.54645401  0.54645401  0.          0.32274454  0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.41074684  0.          0.          0.          0.31238356\n",
      "   0.          0.          0.          0.          0.2425937   0.41074684\n",
      "   0.          0.41074684  0.41074684  0.41074684]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer=TfidfTransformer()\n",
    "tfidf=transformer.fit_transform(X)\n",
    "word=vectorizer.get_feature_names()\n",
    "for w in word:\n",
    "    print(w,end=' ') \n",
    "print()\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.06049928  0.12087772]\n",
      " [ 0.06049928  1.          0.07829579]\n",
      " [ 0.12087772  0.07829579  1.        ]]\n",
      "[[ 11.   1.   2.]\n",
      " [  1.   4.   1.]\n",
      " [  2.   1.   7.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "print(linear_kernel(tfidf,tfidf))\n",
    "\n",
    "print(linear_kernel(X,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.06049928  0.12087772]]\n",
      "[ 1.          0.06049928  0.12087772]\n",
      "[ 1.          0.06049928  0.12087772]\n",
      "[ 11.   1.   2.]\n"
     ]
    }
   ],
   "source": [
    "print(linear_kernel(tfidf[0],tfidf))\n",
    "print(linear_kernel(tfidf[0],tfidf).flatten()) # flatten() 去一維\n",
    "cosine_similarities=linear_kernel(tfidf[0],tfidf).flatten()\n",
    "print(cosine_similarities)\n",
    "\n",
    "cosine_similarities=linear_kernel(X[0],X).flatten()\n",
    "print(cosine_similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【更新】柯P：洪智坤洩漏公文案還沒看到公文今處理 11.0\n",
      "人事處議處洪智坤柯P：不清楚議處結果 2.0\n",
      "留洪智坤柯：殘障求職不易 1.0\n"
     ]
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "for doc in related_docs_indices[0:]:\n",
    "    print(ary[doc], cosine_similarities[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "import pandas as pd\n",
    "with lite.connect('news.sqlite') as db:\n",
    "    cur = db.cursor()\n",
    "    cur.execute('select * from news_entry;')\n",
    "    data = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba \n",
    "\n",
    "corpus = []\n",
    "titles = []\n",
    "for rec in data:    \n",
    "#     print(rec[4],rec[2])\n",
    "#     break;\n",
    "    corpus.append(' '.join(jieba.cut(rec[2])))\n",
    "    titles.append(rec[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vectorizer=CountVectorizer()\n",
    "transformer=TfidfTransformer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "tfidf=transformer.fit_transform(X)\n",
    "weight=tfidf.toarray()\n",
    "word=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for w in word:\n",
    "#     print(w,end=' ') \n",
    "# print()\n",
    "# print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "詐領伙食費　前海巡大隊長「變」秘書\n",
      "【不只鄭捷】羅螢雪今晚《紅心Ａ》大爆料\n",
      "【更新】汽車炸彈衝市場　巴格達64死\n",
      "【有片】2男同時墜樓身亡　腦漿四溢嚇壞路...\n",
      "Apple Pay登台　銀行估12月可上...\n",
      "【GAME啥】重裝出擊　黃金球追追追\n",
      "鄭捷父親筆授權　後事由葬儀社全權處理\n",
      "【央廣RTI】三菱油耗造假  在日銷售車...\n",
      "【公庫】歷史上的今天：白衣天使，天天做到...\n",
      "【有片】火速槍決　解青雲母問鄭捷雙親「你...\n",
      "中國電動車帶頭衝　長園科終於賺錢\n",
      "【有片有卦】羅瑩雪拒答Ａ濫　原來是這件事...\n",
      "【壹週刊】世新調漲學費爆衝突　校方這麼說\n",
      "Apple Pay要來了　那Samsun...\n",
      "中研院改院長投票辦法未發布　院方：合法\n",
      "世新今衝突的真相？抗議的學生干擾會議引衝...\n",
      "鄭捷昨晚伏法　女法醫：讓這一切結束吧\n",
      "【更新】綁Apple Pay　5步驟搞定\n",
      "【有片】這個小三有夠嗆　一句話讓正宮都認...\n",
      "男闖北市刑大　遭霹靂警過肩摔制伏\n",
      "\n",
      "【更新】汽車炸彈衝市場　巴格達64死\n"
     ]
    }
   ],
   "source": [
    "for title in titles[0:20]:\n",
    "    print(title)\n",
    "print()\n",
    "print(titles[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.73529490e-02   8.65164790e-03   3.26956876e-02   2.92714097e-03\n",
      "   1.00000000e+00   1.99335651e-03   1.35576806e-02   5.96772254e-03\n",
      "   0.00000000e+00   9.13302863e-03   1.38950036e-02   5.16622321e-03\n",
      "   1.58818309e-02   3.03110834e-01   3.33833820e-02   1.82092100e-02\n",
      "   7.51189250e-03   3.15364051e-01   4.02509751e-03   1.50772852e-03\n",
      "   7.85589376e-03   3.30801289e-03   1.48951666e-03   3.68948562e-03\n",
      "   3.18189481e-03   4.52137195e-03   2.07928083e-03   8.90272048e-03\n",
      "   7.46344995e-03   2.93942613e-02   1.48951666e-03   3.68948562e-03\n",
      "   3.18189481e-03   4.52137195e-03   2.07928083e-03   8.90272048e-03\n",
      "   7.46344995e-03   2.93942613e-02   4.35581081e-03   4.34825899e-03\n",
      "   5.24901332e-03   9.87293272e-03   2.35109472e-04   2.13265339e-02\n",
      "   5.58222675e-03   7.59843942e-03   1.81637604e-03   8.26769811e-03\n",
      "   4.60786696e-03   7.74884311e-04   0.00000000e+00   2.16776063e-02\n",
      "   1.30822313e-02   6.53420582e-03   1.15276968e-02   5.47605036e-03\n",
      "   2.42014787e-03   1.07973070e-02   0.00000000e+00   6.18181008e-03\n",
      "   3.58553215e-02   6.85185571e-03   1.02761561e-02   7.62958148e-03\n",
      "   3.89726843e-03   5.97698950e-03   1.16575207e-02   1.17070562e-02\n",
      "   7.11098752e-03   1.64406833e-03   1.98099438e-02   4.11970208e-01\n",
      "   1.27490704e-02   3.75898725e-04   1.28157724e-02   2.69914966e-02\n",
      "   7.93190114e-03   2.18897336e-01   5.47303298e-03   3.74340386e-01\n",
      "   1.59541936e-03   5.27451795e-03   2.73597295e-03   1.76916064e-02\n",
      "   4.04192474e-02   0.00000000e+00   1.19773530e-02   8.54146828e-03\n",
      "   3.14810090e-03   5.78494896e-03   2.71030902e-04   9.17560645e-03\n",
      "   1.15841847e-02   1.57100863e-02   1.75055809e-02   2.13811783e-03\n",
      "   0.00000000e+00   1.24362384e-02   2.12636260e-03   9.29508223e-03\n",
      "   4.59271059e-03   1.79851821e-02   0.00000000e+00   2.64222013e-03\n",
      "   1.37357401e-02   8.41361999e-03   1.58004636e-02   1.34988120e-02\n",
      "   7.85511190e-03   4.32498001e-03   4.58707394e-01   8.92620940e-03\n",
      "   3.67865742e-03   2.20565227e-04   3.09491798e-01   9.57740570e-03\n",
      "   2.60398201e-03   3.58001411e-02   7.19505611e-03   1.29142978e-03\n",
      "   5.19804213e-03   2.23699021e-03   0.00000000e+00   3.54175245e-03\n",
      "   5.82400173e-03   4.05276266e-03   1.90049207e-02   1.56062882e-04\n",
      "   3.06119958e-03   7.82423184e-03   2.48471254e-02   6.05069071e-03\n",
      "   6.18423122e-03   4.98734956e-03   1.21951715e-02   9.68055292e-03\n",
      "   2.16504812e-03   1.07502511e-02   3.25329065e-04   5.35166796e-03\n",
      "   1.84039238e-04   8.76776361e-03   4.63463202e-03   5.78695985e-03\n",
      "   0.00000000e+00   5.68129061e-03   1.73248863e-02   5.47640420e-04\n",
      "   9.51116331e-03   1.04698725e-02   1.08004881e-03   0.00000000e+00\n",
      "   4.69622806e-03   1.63649206e-02   2.46788773e-02   6.76154305e-03\n",
      "   8.38118561e-03   8.64988833e-03   1.06556907e-02   3.69012107e-03\n",
      "   7.71436897e-03   6.37696799e-03   0.00000000e+00   2.77566373e-02\n",
      "   1.81203752e-03   1.04031625e-02   3.26052955e-02   9.71114967e-03\n",
      "   8.85543615e-03   2.98099355e-03   4.71651376e-03   9.64262496e-02\n",
      "   5.83807238e-03   1.54845823e-02   3.08254978e-02   5.62471207e-03\n",
      "   7.62361621e-03   2.30038696e-03   7.79615545e-03   1.25577322e-03\n",
      "   7.62361621e-03   2.30038696e-03   7.79615545e-03   1.25577322e-03\n",
      "   1.62806653e-02   9.12957415e-03   4.32953489e-03   1.90717580e-02\n",
      "   1.85615702e-02   3.21582960e-03   1.95244762e-02   1.48790523e-02\n",
      "   2.41659271e-03   5.09891186e-03   1.94444783e-01   1.32937381e-02\n",
      "   1.56787732e-02   8.32211665e-03   3.97778277e-04   1.10945767e-02\n",
      "   6.21427482e-02   2.07426160e-02   2.00480015e-03   3.57666144e-03\n",
      "   6.28814767e-03   9.55767409e-03   1.91808978e-02   1.88001339e-02\n",
      "   8.38763475e-03   1.03487360e-03   1.19666112e-02   5.57561701e-03\n",
      "   7.68018618e-03   1.34965349e-02   3.33921949e-03   5.24396782e-03\n",
      "   1.12177271e-02   0.00000000e+00   9.98188391e-03   1.51864750e-02\n",
      "   5.70362111e-03   3.98504824e-03   2.36560130e-03   3.89362031e-03\n",
      "   3.20311689e-03   2.01418992e-02   2.29922571e-02   6.56535280e-03\n",
      "   9.05316204e-04   1.13012623e-02   6.11098200e-03   2.03022792e-04\n",
      "   4.15993991e-03   6.12847966e-03   1.70383388e-02   1.78207586e-03\n",
      "   3.27480265e-04   1.80545760e-01   3.15849173e-03   2.73347260e-03\n",
      "   3.15849173e-03   2.73347260e-03   1.86501306e-02   1.01616123e-02\n",
      "   7.00342720e-03   4.18877857e-03   5.75303144e-03   4.68278126e-03\n",
      "   6.94357948e-03   1.02312049e-03   3.60740729e-03   6.26517784e-03\n",
      "   1.74579454e-02   2.18448132e-02   6.73950737e-03   1.91145774e-04\n",
      "   1.60877241e-02   3.29023939e-03   3.07675654e-03   7.63238042e-03\n",
      "   7.06045439e-03   2.23587730e-04   1.29732233e-03   2.64926938e-02\n",
      "   3.36609326e-04   4.27287976e-03   4.79326596e-03   1.01558594e-02\n",
      "   3.05851363e-03   1.17751149e-02   1.17751149e-02   2.49271347e-03\n",
      "   9.15813998e-03   1.42131858e-02   1.15812756e-02   5.44917007e-03\n",
      "   2.33984258e-02   0.00000000e+00   4.18184440e-03   2.82217570e-03\n",
      "   4.79296418e-03   2.10131294e-02   3.51714954e-02   5.82130639e-03\n",
      "   2.80190608e-04   1.81646913e-02   5.16290759e-03   2.43486709e-02\n",
      "   1.83476741e-02   2.85422745e-03   2.55939957e-03   4.78430825e-03\n",
      "   5.12976845e-03   1.79947775e-02   2.19591744e-03   2.27382100e-02\n",
      "   1.95939746e-02   2.84694564e-04   1.52014997e-03   6.97462170e-03\n",
      "   1.67584636e-02   3.01584689e-03   4.97679612e-03   1.78674965e-03\n",
      "   4.88236834e-04   7.65401662e-03   3.76151025e-04   3.77444502e-03\n",
      "   6.17693678e-03   2.09487876e-02   5.97897927e-03   3.40919350e-02\n",
      "   5.60377352e-03   8.49274786e-03   8.75489540e-03   3.26336098e-03\n",
      "   1.07151309e-02   2.73167782e-03   8.43479043e-03   5.81860286e-03\n",
      "   1.11292715e-02   6.58696078e-03   3.82492384e-02   2.03981450e-03\n",
      "   1.62931147e-02   0.00000000e+00   3.64083687e-03   5.46434800e-04\n",
      "   8.18549006e-03   6.18987514e-03   4.25603258e-03   3.79305492e-03\n",
      "   6.51855466e-03   9.05308158e-03   3.09892141e-04   3.11607936e-02\n",
      "   2.66698813e-03   9.82469945e-02   3.34942403e-03   4.80194557e-03\n",
      "   0.00000000e+00   6.74989420e-03   4.80580002e-03   3.05322333e-03\n",
      "   0.00000000e+00   8.53020269e-03   5.20607174e-03   0.00000000e+00\n",
      "   4.40265715e-04   4.77132638e-03   3.51062593e-04   8.17617215e-03\n",
      "   4.97356785e-03   5.51065950e-03   2.95807201e-03   2.52789989e-04\n",
      "   4.79215457e-02   8.06232297e-03   1.29536550e-02   1.82784718e-02\n",
      "   8.93640668e-03   1.87024987e-02   2.36103153e-02   4.38627059e-03\n",
      "   3.04980599e-03   1.46739587e-03   8.14030982e-03   4.41459717e-03\n",
      "   2.28960355e-02   7.09433105e-03   7.05303482e-03   7.67051923e-03\n",
      "   2.89636694e-04   2.16462373e-03   1.06052937e-02   2.76911044e-03\n",
      "   2.26702192e-02   3.99171879e-03   3.49258159e-04   1.92711390e-03\n",
      "   1.37200430e-02   1.10897848e-02   3.19094594e-04   7.00420712e-03\n",
      "   5.84541627e-03   2.44089379e-04   2.92563899e-03   3.33190979e-03\n",
      "   1.26585367e-02   6.37602455e-03   2.97519078e-03   4.24563834e-03\n",
      "   1.68804025e-02   4.34917729e-02   3.72301520e-04   1.92614638e-03\n",
      "   1.08222671e-02   1.29407156e-02   1.95775822e-02   6.82526586e-03\n",
      "   3.07995887e-03   5.99467847e-03   1.73465422e-02   1.35580420e-02\n",
      "   3.15340298e-02   3.49148533e-03   1.03578287e-02   5.38604368e-03\n",
      "   7.73289177e-03   3.80699689e-03   6.84279280e-03   8.48130133e-03\n",
      "   2.89142451e-04   6.14576159e-03   9.14413459e-03   8.51205840e-03\n",
      "   9.41266202e-03   2.81751072e-02   8.30257199e-03   6.37908843e-03\n",
      "   6.37908843e-03   6.41627847e-03   0.00000000e+00   2.13508822e-02\n",
      "   2.15956778e-03   1.42005921e-02   1.64381546e-02   1.10699446e-02\n",
      "   3.04081933e-02   6.28706895e-03   1.25111177e-02   7.30580553e-03\n",
      "   7.09369888e-03   4.49250320e-04   2.58942585e-04   3.15504061e-04\n",
      "   4.03355378e-04   1.49657614e-02   1.22224805e-02   9.01453916e-03\n",
      "   4.10836133e-03   5.70996126e-03   8.47497735e-03   3.63612405e-04\n",
      "   5.72572291e-02   2.30811253e-04   3.19355160e-02   2.75184987e-03\n",
      "   2.15079754e-03   5.22606278e-03   7.12836154e-04   5.86812101e-03\n",
      "   2.53248642e-02   1.37736728e-02   2.21999791e-02   1.20479141e-02\n",
      "   1.58376476e-02   6.84498651e-04   1.02543376e-02   1.49246294e-02\n",
      "   6.01866827e-03   2.49649624e-03   1.52372439e-03   5.42960418e-03\n",
      "   0.00000000e+00   5.33360171e-03   1.07882005e-02   1.58403484e-02\n",
      "   1.19496777e-02   7.19588844e-03   3.12471373e-03   1.10897352e-02\n",
      "   6.58259268e-03   6.50723160e-03   3.78066278e-03   1.04454970e-02\n",
      "   6.86834819e-04   6.50241232e-03   7.01675502e-03   4.79221026e-03\n",
      "   4.79221026e-03   1.11372649e-02   3.18329511e-03   2.65921190e-03\n",
      "   2.51642208e-02   3.10552209e-04   4.25000142e-03   4.78007433e-03\n",
      "   3.60495888e-04   1.05192885e-02   5.27793474e-03   1.84197384e-02\n",
      "   2.35182686e-03   4.27954382e-03   7.43653850e-03   1.04875799e-02\n",
      "   1.90556219e-03   8.03679056e-03   6.64035580e-03   9.80999107e-03\n",
      "   2.57000099e-02   2.45457370e-03   5.54263310e-03   1.11605377e-02\n",
      "   1.24287615e-02   2.59099590e-03   4.52729746e-03   3.60009157e-03\n",
      "   1.83361765e-02   2.06250010e-02   3.60009157e-03   1.83361765e-02\n",
      "   2.06250010e-02   1.56178669e-02   8.79857546e-03   4.56587970e-04\n",
      "   9.22472190e-03   0.00000000e+00   3.68553933e-03   2.32420545e-03\n",
      "   1.23555218e-02   1.09516577e-02   2.58187140e-03   2.61993181e-03\n",
      "   2.58607901e-02   8.60317606e-03   2.10105751e-04   2.53470495e-04\n",
      "   4.13911579e-04   6.91122548e-03   1.34409698e-02   7.69103746e-03\n",
      "   3.35837719e-03   1.26114019e-02   5.75999840e-03   2.07654513e-02\n",
      "   2.15676411e-04   9.03483543e-03   1.73099670e-02   5.41904055e-03\n",
      "   2.62746467e-03   2.98362874e-03   6.45867434e-03   2.05829577e-03\n",
      "   5.46091160e-03   6.43405833e-03   1.83502856e-03   8.10992644e-03\n",
      "   8.93703116e-04   1.50607179e-02   4.25687190e-04   6.21644627e-03\n",
      "   3.99061902e-03   0.00000000e+00   3.22771661e-03   1.02662388e-02\n",
      "   3.95192312e-03   2.60767400e-02   7.48169095e-03   2.90100371e-04\n",
      "   7.24506107e-04   9.41936853e-03   7.96647873e-03   6.91464098e-04\n",
      "   4.86965809e-02   1.06439596e-02   3.58662129e-03   2.86306905e-03\n",
      "   6.13773286e-03   5.62835185e-04   3.58662129e-03   2.86306905e-03\n",
      "   6.13773286e-03   5.62835185e-04   6.33936127e-02   1.95739496e-02\n",
      "   1.69172198e-02   2.64383221e-02   2.89105563e-04   1.17496776e-02\n",
      "   6.23791822e-04   4.47023594e-03   1.06250026e-02   1.68629394e-02\n",
      "   8.50959366e-04   1.47109175e-02   5.23609411e-04   1.72669997e-02\n",
      "   1.97943173e-02   4.24199894e-03   2.82311851e-03   3.00485417e-03\n",
      "   1.32753320e-02   1.48478347e-03   6.86867139e-03   8.81052123e-03\n",
      "   5.15180863e-03   1.15023834e-02   3.88690976e-03   5.66315748e-03\n",
      "   5.67536444e-03   1.33837756e-02   3.81080480e-03   2.46151726e-03\n",
      "   2.19322953e-02   7.22962978e-03   1.09496334e-03   3.18312180e-04\n",
      "   4.96716330e-03   1.70557354e-04   1.26632188e-02   4.67140852e-03\n",
      "   2.33337075e-02   9.14411910e-03   4.45868356e-04   4.01719016e-03\n",
      "   8.68422376e-03   1.62147107e-02   5.07974921e-03   1.44484148e-02\n",
      "   4.21387270e-03   1.03706902e-02   6.67552282e-03   1.44136435e-03\n",
      "   3.58294202e-03   4.18772174e-03   7.15956936e-03   1.88604145e-02\n",
      "   2.30009655e-03   1.82201700e-03   1.62025659e-02   9.58082148e-03\n",
      "   2.00746565e-03   1.52641041e-03   2.36279194e-02   1.77292814e-03\n",
      "   2.55593235e-03   6.44349688e-03   2.92191546e-02   1.44926669e-02\n",
      "   1.41315334e-02   3.80072655e-03   0.00000000e+00   4.51504547e-03\n",
      "   1.26641770e-02   3.22033873e-02   5.03377317e-03   4.39008076e-03\n",
      "   6.52184437e-04   3.83679325e-03   4.38476279e-04   9.96802692e-03\n",
      "   2.76336847e-02   1.20895250e-02   1.96848044e-02   5.43007434e-03\n",
      "   2.62854097e-04   2.41688549e-03   6.20034878e-04   2.22906340e-03\n",
      "   4.60681548e-03   3.11336373e-03   1.53754829e-02   4.40457764e-03\n",
      "   1.75545799e-02   4.13644663e-03   2.94295587e-03   9.66440452e-03\n",
      "   5.43644463e-04   6.74571615e-03   5.29574576e-03   9.05829446e-03\n",
      "   6.06225591e-03   8.64997770e-03   4.46530914e-03   5.33879478e-03\n",
      "   4.27733256e-03   9.03126090e-03   3.67531645e-04   6.49692372e-03\n",
      "   3.37516328e-03   1.09816182e-02   7.98157015e-04   1.91186704e-03\n",
      "   1.41393306e-02   3.12077091e-03   1.38073824e-02   5.82811725e-03\n",
      "   1.20585776e-02   3.77889605e-03   7.04991225e-03   7.52587500e-03\n",
      "   1.38737490e-02   1.40972660e-02   5.36738103e-03   5.74264502e-03\n",
      "   4.21091669e-03]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tfidf[4],tfidf).flatten()\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Pay登台　銀行估12月可上... 1.0\n",
      "全球6國開放Apple Pay　亞洲僅中... 0.458707393501\n",
      "【有片大補帖～】Apple Pay這樣用 0.41197020772\n",
      "Apple Pay來台　交易虛擬號碼To... 0.374340385844\n",
      "【更新】綁Apple Pay　5步驟搞定 0.31536405112\n",
      "Apple Pay放行　銀行業者：年底可... 0.309491798338\n",
      "Apple Pay要來了　那Samsun... 0.303110833585\n",
      "【更新】用Apple Pay記得先開小額... 0.218897335689\n",
      "Apple pay登台　行政院長張善政下... 0.19444478298\n",
      "台新信用卡當電子發票　大潤發也適用 0.18054576007\n"
     ]
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "for doc in related_docs_indices[0:]:\n",
    "    if cosine_similarities[doc] >= 0.1:\n",
    "        print(titles[doc], cosine_similarities[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "def getSimiliarArticle(idx):\n",
    "    print(\"[查詢文章]:\" , titles[idx])\n",
    "    cosine_similarities = linear_kernel(tfidf[idx],tfidf).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "    for doc in related_docs_indices[1:]:\n",
    "        if cosine_similarities[doc] >= 0.1:\n",
    "            print(\"[相似文章]:\", titles[doc], cosine_similarities[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[查詢文章]: Apple Pay登台　銀行估12月可上...\n",
      "[相似文章]: 全球6國開放Apple Pay　亞洲僅中... 0.458707393501\n",
      "[相似文章]: 【有片大補帖～】Apple Pay這樣用 0.41197020772\n",
      "[相似文章]: Apple Pay來台　交易虛擬號碼To... 0.374340385844\n",
      "[相似文章]: 【更新】綁Apple Pay　5步驟搞定 0.31536405112\n",
      "[相似文章]: Apple Pay放行　銀行業者：年底可... 0.309491798338\n",
      "[相似文章]: Apple Pay要來了　那Samsun... 0.303110833585\n",
      "[相似文章]: 【更新】用Apple Pay記得先開小額... 0.218897335689\n",
      "[相似文章]: Apple pay登台　行政院長張善政下... 0.19444478298\n",
      "[相似文章]: 台新信用卡當電子發票　大潤發也適用 0.18054576007\n"
     ]
    }
   ],
   "source": [
    "getSimiliarArticle(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering news with kemans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba \n",
    "\n",
    "corpus = []\n",
    "titles = []\n",
    "for rec in data[0:300]:\n",
    "    corpus.append(' '.join(jieba.cut(rec[2])))\n",
    "    titles.append(rec[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "c = cluster.KMeans(n_clusters=6)\n",
    "k_data = c.fit_predict(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【更新】汽車炸彈衝市場　巴格達64死\n",
      "媽媽大掃除搜出子彈　兒握手中突爆炸\n",
      "媽媽大掃除搜出子彈　兒握手中突爆炸\n",
      "未注意高度的結果　拖板車卡涵洞GG了\n",
      "《公投法》立院初審共識　大幅調降公投門檻\n",
      "【醫藥新知】低脂飲食改善癌症病人胃腸道症...\n",
      "注意！　南投縣有大雷雨\n",
      "公所舉辦龍舟賽　村長竟A了姪子工資\n",
      "【壹週刊】太空場景滑沙　越南美奈賞沙丘\n",
      "今部分地區下大雷雨　鄭明典：注意落雷\n",
      "要下大雨了！雲林、嘉義等4縣市發布大雨特...\n",
      "「最美鎮長」涉貪　媽媽也曾涉賄停職\n",
      "對流旺盛　南投、嘉義注意大雷雨\n",
      "蔡英文恭喜菲國新總統勝選　強調深化雙向交...\n",
      "上周災防通報簡訊出包　NCC：民眾應速更...\n",
      "【央廣RTI】不會要他道歉  歐巴馬將訪...\n",
      "【有片】 激戰！伊拉克反攻摩蘇爾\n",
      "Google：並無於彰濱成立研發中心計劃\n",
      "新竹巨城商圈　房市交易住宅熱店面冷\n",
      "對流雲系旺盛　氣象局：嘉義縣要注意大雷雨\n",
      "對流發展旺盛　雲林至高雄防大雨與雷擊\n"
     ]
    }
   ],
   "source": [
    "for idx, group in enumerate(k_data):\n",
    "    if group == 0:\n",
    "        print(titles[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "from xml.dom import minidom\n",
    "from xml.etree import ElementTree\n",
    "import jieba.analyse\n",
    "\n",
    "f = open('1435449602.xml', 'r')\n",
    "events=ElementTree.fromstring(f.read())\n",
    "f.close()\n",
    "\n",
    "corpus = []\n",
    "ary    = []\n",
    "src    = []\n",
    "print(len(events.findall('./channel/item')))\n",
    "for elem in events.findall('./channel/item'):\n",
    "    guid        = elem.find('guid').text\n",
    "    title       = elem.find('title').text\n",
    "    description = elem.find('description').text\n",
    "    pubDate     = elem.find('pubDate').text\n",
    "    source      = elem.find('source').text\n",
    "    src.append(source)\n",
    "    ary.append(title)\n",
    "    corpus.append(' '.join(jieba.analyse.extract_tags(description, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "c = cluster.KMeans(n_clusters=6)\n",
    "k_data = c.fit_predict(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江蕙得「特別貢獻獎」 感恩金曲肯定她\n",
      "羅志祥哭了 蔡依林讚表現很好\n",
      "蔡依林淚奪金曲 錦榮傳訊恭喜\n",
      "德國即將關閉使用最久核子反應爐\n",
      "陳奕迅、張惠妹稱王封后  蔡依林抱回最大獎\n",
      "【美洲盃八強】巴西 VS. 巴拉圭 線上直播！\n",
      "陳奕迅、莫文蔚伴侶均不知阿娜答金曲獲獎\n",
      "北美》線上貸款業搶飯碗\n",
      "一周大事－6月21日至27日\n",
      "超商父親節早鳥預購開跑　聶永真設計商品再次收藏\n",
      "金曲紅毯眾星出招　個人意見獨特見解\n",
      "金曲26／陳奕迅二度擊敗張學友　濕身奪歌王\n",
      "金曲26／蔡依林擒３獎大勝　淚崩再挺婚姻平權\n",
      "金曲26／張惠妹奪歌后卻失落　要世界感受彩虹力量\n",
      "金曲26／蔡依林淚奪最佳專輯＋完整得獎名單\n",
      "僅次Jolin！徐佳瑩入圍6獎全槓被封遺珠\n",
      "金曲最風光！蔡依林紅毯全勝又獲3獎成大贏家\n",
      "張惠妹3度封后  想破江蕙紀錄\n",
      "金曲26／陳奕迅稱王謝台灣　張惠妹封后秒噴淚\n",
      "蔡依林呸大贏家  金曲最佳專輯獎\n",
      "陳奕迅二度打敗歌神  金曲歌王好嗨\n",
      "大人物攝影展開展吸人潮 方念華 時尚入境\n",
      "金曲獎完整得獎名單！阿妹封后 陳奕迅稱王\n",
      "第26屆金曲獎 陳奕迅奪歌王、阿妹封歌后\n",
      "金曲最佳國語專輯：呸\n",
      "《金曲26》2015金曲獎得獎名單 線上直播懶人包\n"
     ]
    }
   ],
   "source": [
    "for idx, group in enumerate(k_data):\n",
    "    if group == 1:\n",
    "        print(ary[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "import jieba\n",
    "import pandas as pd\n",
    "tags = []\n",
    "corpus = []\n",
    "titles = []\n",
    "with lite.connect('news_0506.sqlite') as db:\n",
    "    cur = db.cursor()\n",
    "    cur.execute('select summary, title, category from news_entry;')\n",
    "    data = cur.fetchall()\n",
    "    for rec in data:\n",
    "        if rec[2] in ['娛樂', '社會', '財經']:\n",
    "            corpus.append(' '.join(jieba.cut(rec[0])))\n",
    "            titles.append(rec[1])\n",
    "            tags.append(rec[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))\n",
    "# for ele, r, c in zip(tags, titles, corpus):\n",
    "#    print(ele,r,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "X =vectorizer.fit_transform(corpus)\n",
    "word =vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_data,test_data,train_tag,test_tag, train_title, test_title=train_test_split(X,tags,titles,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "print(len(train_tag))\n",
    "print(len(test_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf= MultinomialNB(alpha=0.01)\n",
    "clf.fit(train_data,train_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "娛樂 社會 財經 \n",
      "[[33  0  2]\n",
      " [ 0 75  1]\n",
      " [ 0  0 33]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_tag,pred)\n",
    "\n",
    "for ele in clf.classes_:\n",
    "    print(ele,end=' ')\n",
    "print()\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "娛樂 財經 超吸金！《美國隊長3》全台首周賣破3億\n",
      "社會 財經 【公庫】旗山大溝頂老街 反政府拆遷\n",
      "娛樂 財經 葉樹姍回鍋大愛台　接棒總監\n"
     ]
    }
   ],
   "source": [
    "for t, p, title in  zip(test_tag, pred, test_title):\n",
    "    if t != p:\n",
    "        print(t, p, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979166666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_tag, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
